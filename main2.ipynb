{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ME 233 Final Project: How to house</h1>\n",
    "<h3>By Jesus Cervantes, Nicholas Robles, Joanna Liu, Isaiah Drummond </h3>\n",
    "\n",
    "This project explores how different ways of assigning incoming students to dorms might impact the spread of COVID-19 on campus udner different quarantine scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package Imports\n",
    "Put all package imports in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# get_results_by_dorm class code imports:\n",
    "from matplotlib import ticker\n",
    "import os\n",
    "import scipy.io\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data download filename\n",
    "STUDENT_DATA_FILENAME = \"student_state_merged.csv\"\n",
    "\n",
    "#Dorm assignment schemes\n",
    "ASSIGN_UNIFORM = \"UNIFORM\"\n",
    "ASSIGN_BY_RISK = \"RISK\"\n",
    "\n",
    "#Interaction schemes\n",
    "INTERACT_UNIFORM = \"UNIFORM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading State Data\n",
    "Here, we import state-based data including the number of students coming from each state and information about the virus in that state. We then return the merged data file that has the total number of students for the class years of interest and the relevant state-wide data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_file(student_filename, yrs):\n",
    "    # student_filename: string, filename of the table with data about where students are coming from\n",
    "    # yrs: int, col indices for class years to consider for moving in\n",
    "    # state_filename: string, filename of the table with init conditions (S_0, E_0, etc.) by state\n",
    "    \n",
    "    # Add the data table (.csv, .xlsx, etc.) into the git repo, then read it in here\n",
    "    # Do whatever cleaning up needs to be done and return the table\n",
    "    data = pd.read_csv(student_filename)\n",
    "    \n",
    "    # Create data table using information from .csv\n",
    "    student_data = pd.DataFrame(np.zeros((55,8)), columns=[\"State\",\"Population\",\"S_0\",\"E_0\",\"I_0\",\"R_0\",\"Rt\",\"beta\"])\n",
    "    student_data.State = data.State\n",
    "    student_data.Population = data[years_to_consider].sum(axis=1)\n",
    "    student_data.S_0 = data.S_0\n",
    "    student_data.E_0 = data.E_0\n",
    "    student_data.I_0 = data.I_0\n",
    "    student_data.R_0 = data.R_0\n",
    "    student_data.Rt = data.R_t\n",
    "    student_data.beta = data.beta\n",
    "    \n",
    "    return student_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_to_consider = [\"UG1\", \"UG2\"] #Subject to change pending what the state data looks like\n",
    "\n",
    "state_based_data = read_data_from_file(STUDENT_DATA_FILENAME, years_to_consider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make dorms\n",
    "\n",
    "Here, we take our state data and make our dorms out of it. For each dorm, we assign a subset of the student population to be in that dorm. Once we have assigned the students to the dorm, we get a weighted average based on where students came from to get one single risk measure for that dorm (probably based on positivity rate of the state). A major part of our experiment is how we do the assignment, our control is assuming a uniform distribution throughout each dorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dorm_assignments(n_dorms, pop, state_data, assign, dorm_pops=None):\n",
    "    # n_dorms: int, number of dorms\n",
    "    # pop: int, total student population\n",
    "    # state_data: np array or pd dataframe, information by state w # students from state + state pos. rate\n",
    "    # assign: string, determines what kind of assignment mechanism to use\n",
    "    # dorm_pops (Optional): dict mapping dorms to # students in that dorm, not using for now but in case we want to make dorms unique\n",
    "    \n",
    "    # RETURNS: dorms, pd dataframe, each row is a dorm, the columns in order are: # students, S_0, E_0, I_0, R_0, Rt, beta\n",
    "    \n",
    "    # Each elem in dorms is a float that is the weighted risk of that dorm\n",
    "    dorms = pd.DataFrame(np.zeros((n_dorms,7)), columns=[\"Students\", \"S_0\", \"E_0\",\"I_0\", \"R_0\", \"Rt\", \"beta\"])\n",
    "    \n",
    "    # Create size of each dorm such that every dorm has the same population of students.\n",
    "    dorm_pop = pop / n_dorms\n",
    "    \n",
    "    # For the base case, we assign students uniformly to each dorm. Each dorm has same initial conditions\n",
    "    # for SEIR model.\n",
    "    if assign == ASSIGN_UNIFORM:\n",
    "        dorms.Students = dorm_pop\n",
    "        dorms.S_0 = sum(state_data.S_0 * state_data.Population) / pop\n",
    "        dorms.E_0 = sum(state_data.E_0 * state_data.Population) / pop\n",
    "        dorms.I_0 = sum(state_data.I_0 * state_data.Population) / pop\n",
    "        dorms.R_0 = sum(state_data.R_0 * state_data.Population) / pop\n",
    "        dorms.Rt = sum(state_data.Rt * state_data.Population) / pop\n",
    "        dorms.beta = sum(state_data.beta * state_data.Population) / pop\n",
    "        print(\"We are using uniform distribution\")\n",
    "        \n",
    "    # For the alternative case, we order states by risk level and then assign equal number of students to each dorm.\n",
    "    # However, we assign students based on risk level of state (i.e., students from high risk states will live in\n",
    "    # same dorm whereas students from states with lower cases will live in the same space).\n",
    "    elif assign == ASSIGN_BY_RISK:\n",
    "        \n",
    "        # Sort data by beta values from lowest to highest. This will have the students from similar states positioned\n",
    "        # next to one another in the array.\n",
    "        sorted_data = state_data.sort_values(\"beta\", ascending=False).reset_index()\n",
    "    \n",
    "        # Set up counter that progresses through number of dorms and one that progresses through states. Set up \n",
    "        # empty arrays for the weighted averages of SEIR data for each dorm. Set up variable \"current_pop\" which\n",
    "        # holds the number of students in a dorm as we cycle through our loop. This will never exceed dorm_pop.\n",
    "        current_pop = 0\n",
    "        dorm_pos = 0\n",
    "        state_pos = 0\n",
    "        dorm_S0 = np.zeros((n_dorms,1))\n",
    "        dorm_E0 = np.zeros((n_dorms,1))\n",
    "        dorm_I0 = np.zeros((n_dorms,1))\n",
    "        dorm_R0 = np.zeros((n_dorms,1))\n",
    "        dorm_Rt = np.zeros((n_dorms,1))\n",
    "        dorm_beta = np.zeros((n_dorms,1))\n",
    "\n",
    "        # Loop through each state in the list and fit weighted averages of SEIR data to each dorm. Ensure that each dorm\n",
    "        # has the same number of students.\n",
    "        while state_pos < len(sorted_data.State):\n",
    "            \n",
    "            # If the dorm has enough room to fit an entire state, put all the residents in the dorm. Then progress to next state.\n",
    "            if current_pop + sorted_data.Population[state_pos] <= dorm_pop:\n",
    "                dorm_S0[dorm_pos] = dorm_S0[dorm_pos] + (sorted_data.S_0[state_pos] * sorted_data.Population[state_pos]) / pop\n",
    "                dorm_E0[dorm_pos] = dorm_E0[dorm_pos] + (sorted_data.E_0[state_pos] * sorted_data.Population[state_pos]) / pop\n",
    "                dorm_I0[dorm_pos] = dorm_I0[dorm_pos] + (sorted_data.I_0[state_pos] * sorted_data.Population[state_pos]) / pop\n",
    "                dorm_R0[dorm_pos] = dorm_R0[dorm_pos] + (sorted_data.R_0[state_pos] * sorted_data.Population[state_pos]) / pop\n",
    "                dorm_Rt[dorm_pos] = dorm_Rt[dorm_pos] + (sorted_data.Rt[state_pos] * sorted_data.Population[state_pos]) / pop\n",
    "                dorm_beta[dorm_pos] = dorm_beta[dorm_pos] + (sorted_data.beta[state_pos] * sorted_data.Population[state_pos]) / pop\n",
    "\n",
    "                current_pop = current_pop + sorted_data.Population[state_pos]\n",
    "                state_pos = state_pos + 1\n",
    "\n",
    "            # If there are more people in the state than space left in the dorm, fill up the current dorm and progress\n",
    "            # to the next one.\n",
    "            else:\n",
    "                dorm_S0[dorm_pos] = dorm_S0[dorm_pos] + (sorted_data.S_0[state_pos] * (dorm_pop - current_pop)) / pop\n",
    "                dorm_E0[dorm_pos] = dorm_E0[dorm_pos] + (sorted_data.E_0[state_pos] * (dorm_pop - current_pop)) / pop\n",
    "                dorm_I0[dorm_pos] = dorm_I0[dorm_pos] + (sorted_data.I_0[state_pos] * (dorm_pop - current_pop)) / pop\n",
    "                dorm_R0[dorm_pos] = dorm_R0[dorm_pos] + (sorted_data.R_0[state_pos] * (dorm_pop - current_pop)) / pop\n",
    "                dorm_Rt[dorm_pos] = dorm_Rt[dorm_pos] + (sorted_data.Rt[state_pos] * (dorm_pop - current_pop)) / pop \n",
    "                dorm_beta[dorm_pos] = dorm_beta[dorm_pos] + (sorted_data.beta[state_pos] * (dorm_pop - current_pop)) / pop\n",
    "\n",
    "                new_pop = sorted_data.Population[state_pos] - (dorm_pop - current_pop)\n",
    "                sorted_data.at[state_pos,\"Population\"] = new_pop                \n",
    "                current_pop = 0\n",
    "                dorm_pos = dorm_pos + 1     \n",
    "    \n",
    "        # Assign correct SEIR characteristics to the dorm dataframe. Return a table with dorms as rows and weighted \n",
    "        # SEIR information as values. Should be ordered from highest beta value to lowest.\n",
    "        dorms.Students = dorm_pop\n",
    "        dorms.S_0 = dorm_S0\n",
    "        dorms.E_0 = dorm_E0\n",
    "        dorms.I_0 = dorm_I0\n",
    "        dorms.R_0 = dorm_R0\n",
    "        dorms.Rt = dorm_Rt\n",
    "        dorms.beta = dorm_beta    \n",
    "        print(\"We are using risk bucketing for dorm assignments\")\n",
    "\n",
    "    return dorms    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using uniform distribution\n"
     ]
    }
   ],
   "source": [
    "# Input number of dorms that we want to put students in. Set total students to be sum of population of incoming students.\n",
    "n_dorms = 10\n",
    "total_students = state_based_data.Population.sum(axis=0) \n",
    "\n",
    "# Run base case: students are uniformly assigned to dorms without regard for home state.\n",
    "dorms_uniform = get_dorm_assignments(n_dorms, total_students, state_based_data, ASSIGN_UNIFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using risk bucketing for dorm assignments\n"
     ]
    }
   ],
   "source": [
    "#Repeat for other assignment types\n",
    "dorms_by_risk = get_dorm_assignments(n_dorms, total_students, state_based_data, ASSIGN_BY_RISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dorm Interactions\n",
    "Now that we have what we need to know for each individual dorm, we get the dorm interaction table. This table has a row for each dorm and includes its initial conditions as before (S_0, E_0, I_0, R_0, Rt, beta) as well as how that dorm interacts with others. Stretch goal is to have one such table for each dorm and the interaction column is going to be how many people from each dorm visit the dorm who the table is for. For now, though, we will assume uniform interaction among dorms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interaction_tables(dorms, interact_num, interact_type):\n",
    "    # dorms: pd dataframe, table of dorms and their initial conditions\n",
    "    # interact_num: int, num interacting residents\n",
    "    # interact_type: string, scheme of interaction, currently just uniform\n",
    "    \n",
    "    \n",
    "    if interact_type == INTERACT_UNIFORM:\n",
    "        interact_col = np.ones(n_dorms)*interact_num\n",
    "        \n",
    "        interact_df = dorms\n",
    "        \n",
    "        interact_df[\"Incoming Passenger\"] = interact_col\n",
    "        \n",
    "        return interact_df\n",
    "        \n",
    "    return None        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# students leaving each dorm, should maybe make function of dorm population\n",
    "interact_num = 10\n",
    "\n",
    "interact_table_control = get_interaction_tables(dorms_uniform, interact_num, INTERACT_UNIFORM)\n",
    "\n",
    "interact_table_by_risk = get_interaction_tables(dorms_by_risk, interact_num, INTERACT_UNIFORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET RESULTS BY DORM\n",
    "\n",
    "So, now that we have our interaction table, we apply it to each of our dorms to get the SEIR model for each dorm. This is done by pretty much just for looping through the networking code they gave us for each of the dorm systems we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in any helpers here for making the networking code happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Results_By_Dorm(dorm_interactions):\n",
    "    # Read the data table (.csv) that's being passed as the parameter\n",
    "    df_summ = pd.read_csv(dorm_interactions)\n",
    "    print(\"Dorms data sheet\")\n",
    "    print(df_summ)\n",
    "    A_all = df_summ['Incoming Passenger'].to_numpy()\n",
    "    # Calculate the incoming infectious as I_0*Incoming_passenger\n",
    "    df_summ['Incoming Infectious']= df_summ['I_0'].mul(df_summ['Incoming Passenger'])\n",
    "    # Contact rates\n",
    "    fb = df_summ['beta'].to_numpy()\n",
    "    # Number of locations\n",
    "    nodes = len(A_all)\n",
    "    \n",
    "    results = {\"0\": [], \"50\":[], \"75\":[], \"95\":[]}\n",
    "\n",
    "    # Set some initial settings based on the results we want to get\n",
    "    dt = 0.01\n",
    "    Tmax = 30\n",
    "    isteps = int(Tmax/dt)\n",
    "    Af =  2.56430\n",
    "    Cf = 6.5\n",
    "    fa = 1/Af\n",
    "    fc = 1/Cf\n",
    "    \n",
    "    # For each dorm\n",
    "    for d in range(10):\n",
    "        \n",
    "        #%% ############## initialize model parameters and seed ######################\n",
    "        # Define output shape of the SEIR model\n",
    "        Sout= np.empty((isteps,nodes))\n",
    "        Eout= np.empty((isteps,nodes))\n",
    "        Iout= np.empty((isteps,nodes))\n",
    "        Rout= np.empty((isteps,nodes))\n",
    "        # Clear incoming passengers for location of consideration\n",
    "        A_all[d] = 0\n",
    "        \n",
    "        # Percent of quarantine\n",
    "        confirmed = forecasting(0,A_all,fb,df_summ,d,Sout,Eout,Iout,Rout)\n",
    "        confirmed_50 = forecasting(0.5,A_all,fb,df_summ,d,Sout,Eout,Iout,Rout)\n",
    "        confirmed_75 = forecasting(0.75,A_all,fb,df_summ,d,Sout,Eout,Iout,Rout)\n",
    "        confirmed_95 = forecasting(0.95,A_all,fb,df_summ,d,Sout,Eout,Iout,Rout)\n",
    "        \n",
    "        results[\"0\"].append(confirmed)\n",
    "        results[\"50\"].append(confirmed_50)\n",
    "        results[\"75\"].append(confirmed_75)\n",
    "        results[\"95\"].append(confirmed_95)\n",
    "\n",
    "        #%% ################### Plot results #######################################\n",
    "        t = np.arange(0,Tmax, dt)\n",
    "        days = np.arange(0,Tmax, 1)\n",
    "        skip = int(1/dt)\n",
    "        Color_p = sns.color_palette(\"RdBu\", 6)\n",
    "        fig, ax1 = plt.subplots(figsize=(500/72,300/72))\n",
    "        ax1.plot(days, confirmed[::skip],color = Color_p[0],lw=3,zorder=1, label=r'100% Inter-dorm interaction') # \n",
    "        ax1.plot(days, confirmed_50[::skip],color = Color_p[2],lw=3,zorder=1, label=r'50% Inter-dorm interaction') #\n",
    "        ax1.plot(days, confirmed_75[::skip],color = Color_p[4],lw=3,zorder=1, label=r'25% Inter-dorm interaction') # \n",
    "        ax1.plot(days, confirmed_95[::skip],color = Color_p[5],lw=3,zorder=1, label=r'5% Inter-dorm interaction') # \n",
    "        leg = plt.legend(loc='upper left',fontsize='large',frameon=True)\n",
    "        leg.get_frame().set_linewidth(0.0)\n",
    "        ax1.xaxis.set_tick_params(labelsize=14)\n",
    "        ax1.yaxis.set_tick_params(labelsize=14)\n",
    "        plt.ylabel('Confirmed Cases [-]',fontsize=14)\n",
    "        plt.xlabel('Time [days]',fontsize=14)\n",
    "        plt.title('Confirmed Cases in Dorm ' + str(d))\n",
    "        plt.tight_layout()\n",
    "        print(\"Analyzing results for Dorm \" + str(d) + \"...\")\n",
    "    \n",
    "    return confirmed\n",
    "\n",
    "#%% ################## define SEIR forecasting function ############################\n",
    "\n",
    "def forecasting(quarantine_scale,A_all,fb,df_summ,idx,Sout,Eout,Iout,Rout):\n",
    "    # Setting initial conditions and the time we are interested in monitoring\n",
    "    Flag_Iso = False\n",
    "    dt = 0.01\n",
    "    Tmax = 30\n",
    "    isteps = int(Tmax/dt)\n",
    "    Af =  2.56430\n",
    "    Cf = 6.5\n",
    "    fa = 1/Af\n",
    "    fc = 1/Cf\n",
    "    \n",
    "    Sarr = df_summ['S_0'].to_numpy()\n",
    "    Earr = df_summ['E_0'].to_numpy()\n",
    "    Iarr = df_summ['I_0'].to_numpy()\n",
    "    Rarr = df_summ['E_0'].to_numpy()\n",
    "\n",
    "    Daily_normalization = 365  # In this case passenger data are accumulated from 1 year\n",
    "\n",
    "    #apply quarantine to states with incoming infectious passengers higher than mean numbers\n",
    "    mean_incoming_infectious= df_summ['Incoming Infectious'].mean(axis=0)\n",
    "    q_states = df_summ['Incoming Infectious']>mean_incoming_infectious\n",
    "    #\n",
    "    q_scaled_states = np.zeros_like(A_all)\n",
    "    \n",
    "    if Flag_Iso:\n",
    "        q_scaled_states[q_states] = quarantine_scale\n",
    "    else:\n",
    "        q_scaled_states = quarantine_scale* np.ones_like(A_all)\n",
    "   \n",
    "    A_all_scaled = ((1-q_scaled_states) * A_all)/Daily_normalization\n",
    "    \n",
    "    A_all_pop =  A_all/Daily_normalization\n",
    "    pop_daily_add = np.sum(A_all_pop)\n",
    "    \n",
    "    # Population of location of consideration\n",
    "    pop = df_summ['Population'].to_numpy()[idx]\n",
    "    pop_new = pop\n",
    "    \n",
    "    # Define an array to add SEIR values only to location of consideration\n",
    "    Scaffold = np.zeros_like(Rarr)\n",
    "    Scaffold[idx] = 1.0\n",
    "    \n",
    "    for s in range(0,isteps):\n",
    "        # Summing up incoming passenger categories\n",
    "        # Sum of all daily incoming passengers is added to the total population in our region of interest\n",
    "        S_add = np.dot(Sarr,A_all_scaled) * Scaffold/(pop_new)\n",
    "        E_add = np.dot(Earr,A_all_scaled) * Scaffold/(pop_new)\n",
    "        I_add = np.dot(Iarr,A_all_scaled) * Scaffold/(pop_new)\n",
    "        R_add = np.dot(Rarr,A_all_scaled) * Scaffold/(pop_new)\n",
    "          \n",
    "        # Calculating the SEIR model, while adding daily passenger influx\n",
    "        SarrNew = Sarr + (S_add - fb * Sarr * Iarr)* dt \n",
    "        EarrNew = Earr + (E_add + (fb * Sarr * Iarr) - fa*Earr )* dt \n",
    "        IarrNew = Iarr + (I_add + fa * Earr - fc * Iarr)* dt \n",
    "        RarrNew = Rarr + (R_add +  fc * Iarr )* dt \n",
    "    \n",
    "        Sarr, Earr, Iarr, Rarr = SarrNew, EarrNew, IarrNew, RarrNew\n",
    "    \n",
    "        Sout[s,:] = Sarr\n",
    "        Eout[s,:] = Earr\n",
    "        Iout[s,:] = Iarr\n",
    "        Rout[s,:] = Rarr\n",
    "        \n",
    "        pop_new = pop_new + pop_daily_add*dt\n",
    "        \n",
    "    confirmed = pop_new*Iout[:,idx] + pop_new*Rout[:,idx]\n",
    "  \n",
    "    return confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid file path or buffer object type: <class 'pandas.core.frame.DataFrame'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-223-8ac5fb0f04af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdorm_results_control\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_Results_By_Dorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minteract_table_control\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdorm_results_by_risk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_Results_By_dorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minteract_table_by_risk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-222-7c9035736a0e>\u001b[0m in \u001b[0;36mget_Results_By_Dorm\u001b[1;34m(dorm_interactions)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_Results_By_Dorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdorm_interactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Read the data table (.csv) that's being passed as the parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdf_summ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdorm_interactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dorms data sheet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_summ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ME233\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ME233\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[1;31m# though mypy handling of conditional imports is difficult.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[1;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0m\u001b[0;32m    435\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ME233\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"Invalid file path or buffer object type: {type(filepath_or_buffer)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'pandas.core.frame.DataFrame'>"
     ]
    }
   ],
   "source": [
    "dorm_results_control = get_Results_By_Dorm(interact_table_control)\n",
    "\n",
    "dorm_results_by_risk = get_Results_By_dorm(interact_table_by_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating Results\n",
    "Here, we gather up all the dorm-specific data we have to get university-wide results for each assignment mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results(dorm_results):\n",
    "    # dorm_results: pd dataframe or np array, each row is time series of data for a dorm\n",
    "    \n",
    "    #Return: single time series that is weighted average of the dorm_results\n",
    "    \n",
    "    #Do stuff that should basically amount to getting weighted average of all the dorms\n",
    "    \n",
    "    #For each row, multiply the whole row by the pop of that dorm\n",
    "    ####TODO####\n",
    "    \n",
    "    #Sum all the rows together\n",
    "    confirmed_total = np.mean(np.array(dorm_results[\"0\"]), axis=0)\n",
    "    \n",
    "    confirmed_total_50 = np.mean(np.array(dorm_results[\"50\"]), axis=0)\n",
    "    \n",
    "    confirmed_total_75 = np.mean(np.array(dorm_results[\"75\"]), axis=0)\n",
    "    \n",
    "    confirmed_total_95 = np.mean(np.array(dorm_results[\"95\"]), axis=0)\n",
    "    \n",
    "    #%% ################### Plot results #######################################\n",
    "    t = np.arange(0,Tmax, dt)\n",
    "    days = np.arange(0,Tmax, 1)\n",
    "    skip = int(1/dt)\n",
    "    # color palette\n",
    "    Color_p = sns.color_palette(\"RdBu\", 6)\n",
    "    fig, ax1 = plt.subplots(figsize=(500/72,300/72))\n",
    "    ax1.plot(days, confirmed_total[::skip],color = Color_p[0],lw=3,zorder=1, label=r'prediction 0% quarantine') # \n",
    "    ax1.plot(days, confirmed_total_50[::skip],color = Color_p[2],lw=3,zorder=1, label=r'prediction 50% quarantine') #\n",
    "    ax1.plot(days, confirmed_total_75[::skip],color = Color_p[4],lw=3,zorder=1, label=r'prediction 75% quarantine') # \n",
    "    ax1.plot(days, confirmed_total_95[::skip],color = Color_p[5],lw=3,zorder=1, label=r'prediction 95% quarantine') # \n",
    "    leg = plt.legend(loc='upper left',fontsize='large',frameon=True)\n",
    "    leg.get_frame().set_linewidth(0.0)\n",
    "    ax1.xaxis.set_tick_params(labelsize=14)\n",
    "    ax1.yaxis.set_tick_params(labelsize=14)\n",
    "    plt.ylabel('confirmed cases [-]',fontsize=14)\n",
    "    plt.xlabel('time [days]',fontsize=14)\n",
    "    plt.title('Confirmed Cases in Entire Campus')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    #Divide by total pop, we now have weighted average\n",
    "    return aggregated_results/total_students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_agg = aggregate_results(dorm_results_control)\n",
    "\n",
    "by_risk_agg = aggregate_results(dorm_results_by_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregated Analysis and Plots\n",
    "If there's any plots and stuff we wanna make with the aggregated results, put that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
